{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f65eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhelo world\n"
     ]
    }
   ],
   "source": [
    "print(\"hhelo world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2498d88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'model-growth'...\n",
      "remote: Enumerating objects: 141, done.\u001b[K\n",
      "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
      "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
      "remote: Total 141 (delta 46), reused 127 (delta 32), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (141/141), 4.06 MiB | 17.26 MiB/s, done.\n",
      "Resolving deltas: 100% (46/46), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/yashwantram97/model-growth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9a7b1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bbdc59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-growth  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44aa28d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/model-growth/depth-expansion\n"
     ]
    }
   ],
   "source": [
    "%cd model-growth/depth-expansion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4eb7423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPython \u001b[36m3.11.14\u001b[39m\u001b[36m\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
      "\u001b[2mResolved \u001b[1m69 packages\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m65 packages\u001b[0m \u001b[2min 57.82s\u001b[0m\u001b[0m                                           \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m65 packages\u001b[0m \u001b[2min 233ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.13.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.12.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2026.1.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcuda-bindings\u001b[0m\u001b[2m==12.9.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcuda-pathfinder\u001b[0m\u001b[2m==1.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==4.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==1.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.18\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.83\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.3.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvshmem-cu12\u001b[0m\u001b[2m==3.4.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==26.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==23.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2026.1.15\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==5.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyper-slim\u001b[0m\u001b[2m==0.21.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.22.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf86d4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Kernel Availability Report (1B):\n",
      "  CUDA available:       True\n",
      "  HAS_TRITON:           False\n",
      "  HAS_FLA:              False\n",
      "  Triton RMSNorm:       FALLBACK (PyTorch)\n",
      "  Triton Sinkhorn:      FALLBACK (PyTorch)\n",
      "  Triton Sparse Attn:   FALLBACK (PyTorch)\n",
      "  fla GatedDeltaRule:   FALLBACK (Python loop)\n",
      "============================================================\n",
      "============================================================\n",
      "Kernel Availability Report (3B):\n",
      "  CUDA available:       True\n",
      "  HAS_TRITON:           False\n",
      "  HAS_FLA:              False\n",
      "  Triton RMSNorm:       FALLBACK (PyTorch)\n",
      "  Triton Sinkhorn:      FALLBACK (PyTorch)\n",
      "  Triton Sparse Attn:   FALLBACK (PyTorch)\n",
      "  fla GatedDeltaRule:   FALLBACK (Python loop)\n",
      "============================================================\n",
      "============================================================\n",
      "Kernel Availability Report (3B):\n",
      "  CUDA available:       True\n",
      "  HAS_TRITON:           False\n",
      "  HAS_FLA:              False\n",
      "  Triton RMSNorm:       FALLBACK (PyTorch)\n",
      "  Triton Sinkhorn:      FALLBACK (PyTorch)\n",
      "  Triton Sparse Attn:   FALLBACK (PyTorch)\n",
      "  fla GatedDeltaRule:   FALLBACK (Python loop)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "  train.py â€” diagnostic run\n",
      "  Device  : cuda\n",
      "  Steps   : 5000\n",
      "  Batch   : 4   Seq : 512\n",
      "  LR      : 0.0003   Warmup : 100\n",
      "============================================================\n",
      "\n",
      "[INIT] Loading TSAI tokenizer from /content/model-growth/depth-expansion/tokenizer\n",
      "[INIT] Vocab size: 131,075\n",
      "[INIT] Building Kronecker vocab table (may take ~30s)â€¦\n",
      "  vocab: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65536/65536 [00:00<00:00, 280629.41it/s]\n",
      "[INIT] Kronecker table ready.\n",
      "[INIT] Building Model3B (LocalModelConfig)â€¦\n",
      "   ðŸ”§ pf_to_model (8192â†’768) initialized with std=0.000221\n",
      "\n",
      "ðŸ¤– MODEL WITH MEMORY STREAM RECURRENCE INITIALIZED:\n",
      "   ðŸ”„ Recurrence: Stream 3 | Î»_r=0.0789\n",
      "   Vocabulary: 65,536\n",
      "   Hidden Size: 768\n",
      "\n",
      "   ðŸ“ Kronecker Embeddings:\n",
      "      POS_DIM=32 x CHAR_DIM=256 = D=8192\n",
      "      Buffer size: 536.9M (vocab Ã— 8192, non-trainable)\n",
      "      pf_to_model: 6.3M params (8192 Ã— 768)\n",
      "      âš ï¸  Embedding tying NOT possible (8192 â‰  768)\n",
      "\n",
      "   Total Layers: 8\n",
      "   - DeltaNet: 6 layers (75%) - O(N) linear attention\n",
      "   - GSA: 2 layers (25%) - Adaptive sparse\n",
      "\n",
      "   Context Target: 512 tokens (standard RoPE)\n",
      "   Experts: 4 real + 4 null = 8 slots\n",
      "   Top-k: 2 (dynamic, avg 5 with Ï=0.5)\n",
      "   MTP: Disabled\n",
      "\n",
      "   Total Parameters: 126,683,110 (~0.13B)\n",
      "   Active Parameters: ~4.079B (avg 5 experts Ã— top-k routing)\n",
      "[INIT] Parameters : 126,683,110  (126.7 M)\n",
      "[INIT] Memory est : weightsâ‰ˆ0.25 GB (bf16)  Adamâ‰ˆ1.01 GB  KP-tableâ‰ˆ1.07 GB\n",
      "[DATA] TinyStories â€” will stream from roneneldan/TinyStories\n",
      "[INIT] Logging to: training_log.jsonl  +  training_log.txt\n",
      "\n",
      "[TRAIN] ðŸš€  Starting from step 0  â†’  5000\n",
      "\n",
      "README.md: 1.06kB [00:00, 3.44MB/s]\n",
      "step=     0  loss=11.2128  smooth=11.2128  ppl=74071.0  aux=0.2159  mtp=0.0000  ponder=0.0000  gnorm=140.953  lr=3.00e-06  ms/step=33624\n",
      "\n",
      "[TRAIN] âš ï¸   Interrupted at step 17\n",
      "[CKPT] ðŸ’¾  step=17  loss=8.5726  lr=5.10e-05  â†’ checkpoints/train_latest.pt\n",
      "\n",
      "============================================================\n",
      "  Training complete\n",
      "  Steps     : 18\n",
      "  Final loss: 8.5726  (EMA: 10.7669)\n",
      "  Log files : training_log.jsonl  |  training_log.txt\n",
      "============================================================\n",
      "\n",
      "[CKPT] ðŸ’¾  step=18  loss=8.5726  lr=5.10e-05  â†’ checkpoints/train_latest.pt\n"
     ]
    }
   ],
   "source": [
    "!./.venv/bin/python train_cuda.py --batch 4 --seq 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a3029",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c5af46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
