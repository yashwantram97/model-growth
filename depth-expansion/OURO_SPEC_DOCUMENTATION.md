# Ouro-Spec Training System Documentation

## Table of Contents
1. [Overview](#overview)
2. [System Architecture](#system-architecture)
3. [Core Components](#core-components)
4. [Adaptive Recurrence Mechanism](#adaptive-recurrence-mechanism)
5. [Economic Compute Control](#economic-compute-control)
6. [Training Pipeline](#training-pipeline)
7. [Configuration & Hyperparameters](#configuration--hyperparameters)
8. [Usage Guide](#usage-guide)
9. [Monitoring & Diagnostics](#monitoring--diagnostics)

---

## Overview

**Ouro-Spec** is an adaptive transformer language model that implements **per-token variable-depth processing** with economic compute control. Unlike standard transformers that process all tokens with fixed computation, Ouro-Spec allows each token to decide how much computation it needs.

### Key Features
- âœ¨ **Fourier-based semantic embeddings** for richer token representations
- ğŸ”„ **Adaptive recurrent layers** (1-4 passes per token)
- ğŸ’° **Economic compute controller** with dual-ascent pricing
- ğŸ¯ **Per-token independent execution** (not batch-synchronized)
- ğŸ **Mac M1 (MPS) optimized** for Apple Silicon
- ğŸ“Š **Comprehensive telemetry** and diagnostic logging

### Design Philosophy
> **"Easy tokens get fast-tracked, hard tokens get deep thought"**

The model learns to allocate compute where it matters most, balancing accuracy with efficiency through a market-based pricing mechanism.

---

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT TOKENS                      â”‚
â”‚                  [B, T] token IDs                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Hybrid Fourier Embedding â”‚
         â”‚  BPE â†’ Semantic (2048-D)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Projection (2048â†’768)   â”‚
         â”‚      + RMSNorm            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    Baton Injection        â”‚
         â”‚  x[t] += Î»Â·x[t-1]        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  AdaptiveDecoderLayer 0   â”‚
         â”‚   (1-4 recurrent passes)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                    ...  (8 layers)
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  AdaptiveDecoderLayer 7   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   RMSNorm + LM Head       â”‚
         â”‚   Output: [B, T, 50272]   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Core Components

### 1. Hybrid Fourier Embedding

**Location**: `fourier_se_decoder.py` â†’ `HybridEmbeddingTorch`

Converts BPE tokens into semantic Fourier representations:

```python
# Traditional embedding: lookup table [vocab_size, hidden_dim]
# Fourier embedding: encode token string â†’ frequency domain

Token "hello" â†’ ["h","e","l","l","o"] â†’ Fourier Transform â†’ 2048-D vector
```

**Advantages**:
- Captures subword structure semantically
- Similar tokens have similar embeddings
- More information-dense than one-hot lookups

**Configuration**:
- `CHAR_DIM`: 128 (character vocabulary size)
- `POS_DIM`: 16 (positional encoding dimension)
- `D`: 2048 (output dimension)
- `K_SEM`: 1536 (semantic attention dimension)

### 2. Baton Injection

**Location**: Lines 754-764 in `SmolLM.forward()`

Creates temporal state flow between adjacent tokens:

```python
# Formula
injection = sigmoid(lambda_e) * x[t-1]
x[t] = x[t] + injection

# Effect: token t receives "context" from token t-1
# lambda_e: learnable 768-D vector (per-dimension gating)
```

**Purpose**:
- Provides short-term memory across tokens
- Helps model track local context
- Complements attention mechanism

**Diagnostics Logged**:
- `baton_rat`: Magnitude ratio (injection / current state)
- `lam_mu`: Mean gate value

### 3. Adaptive Decoder Layer

**Location**: Lines 512-731

The core innovation - a transformer layer that can execute **multiple recurrent passes**.

#### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: h (hidden state) [B, T, 768]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Static KV Cache      â”‚  â† Computed once
        â”‚ (Keys, Values)       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â–¼â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
    â”ƒ   RECURRENCE LOOP (k=1..4)  â”ƒ
    â”ƒ                             â”ƒ
    â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”ƒ
    â”ƒ  â”‚ Attention (reuse KV)â”‚    â”ƒ
    â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”ƒ
    â”ƒ            â”‚                â”ƒ
    â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”ƒ
    â”ƒ  â”‚   MoE FFN          â”‚    â”ƒ
    â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”ƒ
    â”ƒ            â”‚                â”ƒ
    â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”ƒ
    â”ƒ  â”‚ Compute Features   â”‚    â”ƒ
    â”ƒ  â”‚ (Entropy, Î”, etc.) â”‚    â”ƒ
    â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”ƒ
    â”ƒ            â”‚                â”ƒ
    â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”ƒ
    â”ƒ  â”‚  Continue Gate     â”‚    â”ƒ
    â”ƒ  â”‚  p_k = Ïƒ(f(x))    â”‚    â”ƒ
    â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”ƒ
    â”ƒ            â”‚                â”ƒ
    â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”ƒ
    â”ƒ  â”‚ Gated Update       â”‚    â”ƒ
    â”ƒ  â”‚ h += s * Î±_k * Î”h  â”‚    â”ƒ
    â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”ƒ
    â”ƒ            â”‚                â”ƒ
    â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”ƒ
    â”ƒ  â”‚ Update Survival    â”‚    â”ƒ
    â”ƒ  â”‚ s = s * p_k        â”‚    â”ƒ
    â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”ƒ
    â”ƒ            â”‚                â”ƒ
    â”ƒ    Early exit if <2%       â”ƒ
    â”ƒ    tokens alive?            â”ƒ
    â”ƒ            â”‚                â”ƒ
    â”—â”â”â”â”â”â”â”â”â”â”â”â”â–¼â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Output: h_final    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Key Variables

| Variable | Shape | Description |
|----------|-------|-------------|
| `h` | [B, T, D] | Hidden state (evolves each pass) |
| `s` | [B, T] | **Survival probability** (per-token) |
| `p_k` | [B, T] | **Continue gate** (probability to run pass k+1) |
| `dh_k` | [B, T, D] | Update vector for pass k |
| `alpha[k]` | scalar | Dampening factor: [1.0, 0.7, 0.5, 0.35] |
| `expected_k` | [B, T] | Cumulative survival (soft depth) |
| `exec_per_token` | [B, T] | Hard execution count |

#### Execution Flow

```python
# Initialize
s = ones([B, T])  # All tokens start "alive"
expected_k = zeros([B, T])

for k in [1, 2, 3, 4]:
    # 1. Mark executing tokens
    exec_per_token[s >= halt_thr] = k
    
    # 2. Process (attention + FFN)
    attn_out = self_attn(h)
    dh = alpha[k-1] * (attn_out + mlp(h + attn_out))
    
    # 3. Compute adaptive features
    features = [entropy, top_k_sum, agreement, delta_rel, cos_dir]
    
    # 4. Gate decision
    p_k = continue_head(features)  # [B, T]
    
    # 5. Gated update (only alive tokens contribute)
    h = h + s * dh
    
    # 6. Update survival
    expected_k += s  # Accumulate soft depth
    s = s * p_k      # Decay survival
    
    # 7. Early exit check
    if (s >= halt_thr).mean() < 0.02:  # <2% alive
        break
```

---

## Adaptive Recurrence Mechanism

### Per-Token Features (5 dimensions)

The `continue_head` neural network decides whether to continue based on:

#### 1. **Entropy (H_norm)** - Uncertainty
```python
# Attention entropy per head, averaged
entropy = -sum(attn_weights * log(attn_weights))
H_norm = entropy / log(T)  # Normalize by sequence length
```
- **High entropy** â†’ Model uncertain â†’ Continue
- **Low entropy** â†’ Model confident â†’ Stop

#### 2. **Top-K Sum (S_topk)** - Attention Confidence
```python
# Sum of top 16 attention weights
S_topk = topk(attn_weights, k=16).sum()
```
- **High sum** â†’ Focused attention â†’ May stop
- **Low sum** â†’ Diffuse attention â†’ Continue

#### 3. **Agreement (Agree)** - Head Consensus
```python
# Inverse of attention variance across heads
Agree = 1 - std_over_heads(attn_weights).mean()
```
- **High agreement** â†’ Heads aligned â†’ May stop
- **Low agreement** â†’ Heads diverge â†’ Continue

#### 4. **Relative Update (delta_rel)** - State Change
```python
# How much the state is changing
delta_rel = ||dh|| / (||h|| + Îµ)
```
- **High delta** â†’ State evolving â†’ Continue
- **Low delta** â†’ State converged â†’ Stop

#### 5. **Cosine Direction (cos_dir)** - Update Consistency
```python
# Alignment with previous update
cos_dir = cosine_similarity(dh_current, dh_prev)
```
- **Positive** â†’ Consistent direction â†’ Continue
- **Negative** â†’ Direction flip â†’ May stop

### Continue Head Architecture

```python
# Input: [B, T, 5] features + [16] layer_embedding
# Total: [B, T, 21]

continue_head = Sequential(
    Linear(21, 32),
    GELU(),
    Linear(32, 1),
    Sigmoid()  # Output: probability [0, 1]
)
```

### Survival Dynamics

```python
# Example timeline for a token
Pass 1: s=1.00, pâ‚=0.85 â†’ hâ‚ = hâ‚€ + 1.00 * dhâ‚
Pass 2: s=0.85, pâ‚‚=0.60 â†’ hâ‚‚ = hâ‚ + 0.85 * 0.7 * dhâ‚‚
Pass 3: s=0.51, pâ‚ƒ=0.30 â†’ hâ‚ƒ = hâ‚‚ + 0.51 * 0.5 * dhâ‚ƒ
Pass 4: s=0.15, pâ‚„=0.00 â†’ hâ‚„ = hâ‚ƒ + 0.15 * 0.35 * dhâ‚„

expected_k = 1.00 + 0.85 + 0.51 + 0.15 = 2.51
```

**Key Insight**: `expected_k` is the **soft depth** - differentiable and tracks "compute density".

---

## Economic Compute Control

### The Problem: Compute Explosion

Without constraints, the model learns:
```
More passes â†’ Better accuracy â†’ Always use Kmax=4
```

This defeats the purpose of adaptive depth!

### Solution: Market-Based Pricing

#### Dual-Ascent Controller

```python
# State variable (persists across training)
lambda_p_state = 0.0001  # Initialize at minimum

# Every non-audit step after warmup:
policy_avg_k = mean([layer.expected_k for layer in layers])
lambda_p_state += DUAL_LR * (policy_avg_k - TARGET_K)
lambda_p_state = clamp(lambda_p_state, LAMBDA_MIN, LAMBDA_MAX)

# Ponder penalty (added to loss)
ponder_signal = relu(avg_k - 1.0) / (Kmax - 1.0)  # [0, 1]
p_loss = lambda_p * ponder_signal
```

#### Key Parameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| `TARGET_K` | 1.30 | Desired average depth |
| `DUAL_LR` | 5e-5 | Controller learning rate |
| `LAMBDA_MIN` | 1e-4 | Price floor |
| `LAMBDA_MAX` | 5e-2 | Price ceiling |

#### Controller Dynamics

```
Step 1000: avg_k=2.5, TARGET=1.3 â†’ Î»_p increases (tax goes up)
Step 1100: avg_k=1.8, TARGET=1.3 â†’ Î»_p increases (still overspending)
Step 1200: avg_k=1.4, TARGET=1.3 â†’ Î»_p increases (close to target)
Step 1300: avg_k=1.2, TARGET=1.3 â†’ Î»_p decreases (underspending)
...
Step 5000: avg_kâ‰ˆ1.3, Î»_pâ‰ˆ0.015 (equilibrium reached)
```

### Asymmetric Deadband (Stability Fix)

Once Î»_p stabilizes, activate **asymmetric deadband**:

```python
if lambda_p_state > 0.50 for 100 steps:
    # Enter deadband mode
    k_high = TARGET_K * 1.05  # +5% ceiling
    k_low = TARGET_K * 0.98   # -2% floor
    
    if policy_avg_k > k_high:
        lambda_p_state += DUAL_LR * 0.5 * (policy_avg_k - k_high)
    elif policy_avg_k < k_low:
        lambda_p_state += DUAL_LR * 0.5 * (policy_avg_k - k_low)
    # else: inside band â†’ no update
```

**Purpose**: Prevent oscillations around equilibrium.

### Coupon Reward (Economic Incentive)

**Problem**: Model learns to stop early to avoid ponder penalty, even when more compute would help.

**Solution**: Reward **useful** recurrence.

```python
# Capture pass-1 logits (baseline)
with no_grad():
    logits_1 = model(x, capture_h1=True)

# Forward full model
logits_final = model(x)

# Measure improvement
CE_1 = cross_entropy(logits_1, targets)
CE_final = cross_entropy(logits_final, targets)
delta_ce = relu(CE_1 - CE_final)  # Positive = improvement

# Apply coupon
REWARD_ETA = 2.0
coupon = REWARD_ETA * delta_ce
total_ponder = relu(raw_ponder - coupon)  # Discount penalty

total_loss = lm_loss + total_ponder
```

**Interpretation**:
- If pass 2+ improves loss: get "coupon" (reduced penalty)
- If pass 2+ doesn't help: pay full penalty
- Creates economic incentive to spend compute wisely

### Audit Forcing (Counterfactual Measurement)

**Problem**: If gates learn to stop at k=1, we can't measure if k=2 would help (no gradient signal).

**Solution**: Occasionally force k=2 to measure counterfactual improvement.

```python
AUDIT_PROB = 0.05  # 5% of batches

if random() < AUDIT_PROB:
    force_2 = True  # Override gates
    # Measure Î”CE but DON'T feed to controller
    # (prevents feedback loop)
```

**Key**: Audit data is **separated** from policy data in controller updates.

---

## Training Pipeline

### Phase 1: Force-2 Warmup (Steps 0-800)

```python
force_2 = True  # All tokens run exactly 2 passes
lambda_p = 0.0  # No ponder penalty
```

**Purpose**: 
- Stabilize model initialization
- Learn that recurrence can be useful
- Prevent early collapse to k=1

### Phase 2: Clamped Adaptive (Steps 800-4000)

```python
force_2 = False  # Gates active
p_k = clamp(p_k, min=0.01, max=0.95)  # Safety rails
lambda_p = increasing  # Controller active
```

**Purpose**:
- Gradual transition to full autonomy
- Prevent premature gate collapse
- Lower clamp (0.01) keeps gradients flowing
- Upper clamp (0.95) ensures some early stopping

### Phase 3: Full Adaptive (Steps 4000-10000)

```python
force_2 = audit_only  # Only audit forcing
p_k = unconstrained  # Full gate freedom
lambda_p = equilibrium  # Controller converged
```

**Purpose**:
- Model fully controls its depth
- Equilibrium between accuracy and efficiency

### Training Schedule

| Component | Steps | Configuration |
|-----------|-------|---------------|
| **LR Warmup** | 0-500 | Linear: 0 â†’ 3e-4 |
| **LR Cosine Decay** | 500-10000 | 3e-4 â†’ 3e-5 |
| **Force-2** | 0-800 | Always k=2 |
| **Clamp** | 0-4000 | p âˆˆ [0.01, 0.95] |
| **Ponder Start** | 1200+ | Î»_p active |
| **Audit Forcing** | 1200+ | 5% probability |

### Loss Function Evolution

```python
# Steps 0-800 (Force-2)
loss = CE_loss  # Pure language modeling

# Steps 800-1200 (Transition)
loss = CE_loss + 0 * ponder  # Ponder computed but not trained

# Steps 1200+ (Economic)
raw_ponder = lambda_p * ponder_signal
coupon = REWARD_ETA * relu(CE_1 - CE_final)
net_ponder = relu(raw_ponder - coupon)
loss = CE_loss + net_ponder
```

---

## Configuration & Hyperparameters

### Model Architecture

```python
# Size: ~100M parameters
vocab_size = 50272       # GPT-2 vocabulary
hidden_size = 768        # Model dimension
num_layers = 8           # Decoder layers
num_heads = 12           # Attention heads
intermediate_size = 512  # FFN dimension
K_SEM = 1536            # Semantic attention dim
```

### Recurrence Parameters

```python
Kmax = 4                      # Max passes per layer
alpha = [1.0, 0.7, 0.5, 0.35] # Dampening factors
halt_thr = 0.10               # Token survival threshold
early_exit_frac = 0.02        # Stop when 2% alive
```

### Economic Controller

```python
TARGET_K = 1.30              # Target depth
DUAL_LR = 5e-5               # Controller LR
LAMBDA_MIN = 1e-4            # Price floor
LAMBDA_MAX = 5e-2            # Price ceiling
REWARD_ETA = 2.0             # Coupon multiplier
AUDIT_PROB = 0.05            # Audit frequency
```

### Training Configuration

```python
batch_size = 16
seq_len = 512
learning_rate = 3e-4 â†’ 3e-5  # Cosine decay
warmup_steps = 500
total_steps = 10000
grad_clip = 1.0
```

### Memory Optimization (Mac M1)

```python
os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "1.0"
torch.mps.set_per_process_memory_fraction(1.0)

# Aggressive cleanup
gc.collect()  # Every step
torch.mps.empty_cache()  # Every step
```

---

## Usage Guide

### Installation

```bash
# Required packages
pip install torch transformers datasets tqdm

# Mac M1: Ensure PyTorch with MPS support
pip install torch torchvision torchaudio
```

### Dataset Preparation

Download and prepare SYNTH dataset:

```bash
# Expected structure:
workspace/
â”œâ”€â”€ synth_local/           # Local copy of PleIAs/SYNTH
â”‚   â”œâ”€â”€ data-00000-of-XXXXX.arrow
â”‚   â””â”€â”€ dataset_info.json
â””â”€â”€ new-fourier-test.py
```

### Training Commands

#### Start Fresh Training

```bash
python new-fourier-test.py fourier --seed 42
```

#### Resume from Checkpoint

```bash
# Auto-resume from latest
python new-fourier-test.py fourier

# Resume from specific checkpoint
python new-fourier-test.py fourier --resume checkpoints/fourier_checkpoint_step_0005000.pt
```

#### Override Compute Price

```bash
# Manually set lambda_p (for experiments)
python new-fourier-test.py fourier --set_lambda 0.01
```

#### Disable Recurrence (Baseline)

```bash
python new-fourier-test.py fourier --hard_kill
```

### Pause/Resume

During training, you can gracefully pause:

```bash
# Terminal 1: Training running
python new-fourier-test.py fourier

# Terminal 2: Trigger pause
touch checkpoints/.pause

# Or send signal
kill -SIGINT <pid>
```

The script will:
1. Finish current step
2. Save checkpoint
3. Exit cleanly

Resume with:
```bash
python new-fourier-test.py fourier  # Auto-detects checkpoint
```

---

## Monitoring & Diagnostics

### Log File Format

#### Main Process Line (Every Step)

```
STEP: 5000 | TOT: 2.1234 | LM: 2.0100 | PONDER: 0.0012 (RAW: 0.0050, RWD: 0.0038) | 
LR: 1.50e-04 | GRAD: 0.8234 | Î»_p: 1.23e-02[DB] | CTRL_ERR: +0.015 | P_NR: 2.3% | 
BATON: 0.123 (lam=0.456) | PL/LM: 0.06% | AVGS: exec=1.45, k=1.32, gap=0.13 | 
dt: 245.67ms | tok/sec: 3345.12
```

**Fields Explained**:

| Field | Description | Target Range |
|-------|-------------|--------------|
| `TOT` | Total loss (LM + ponder) | Decreasing |
| `LM` | Language modeling loss | 1.5-3.0 |
| `PONDER` | Net ponder penalty (after coupon) | <0.01 |
| `RAW` | Raw ponder (before coupon) | â€” |
| `RWD` | Coupon reward value | Positive = good |
| `Î»_p` | Current compute price | 0.01-0.05 |
| `[DB]` | Deadband active flag | â€” |
| `CTRL_ERR` | k - TARGET_K | Close to 0 |
| `P_NR` | % gates near collapse (â‰¤0.05) | <5% |
| `exec` | Batch-max passes executed | 1-4 |
| `k` | Average expected depth | ~1.30 |
| `gap` | exec - k (wasted compute) | <0.5 |

#### Layer Statistics (Every 10 Steps)

```
 L0: k_exec=2, expected_k=1.45, s_fin=0.023 | 
 tok: avg=1.32, p95=2.00 | tail: â‰¥2:45%, â‰¥3:12%, â‰¥4:2% | 
 ent=0.1234 | p_mu=0.65, p_std=0.234, d_rel=0.123 | 
 Î”h=0.045, Î”p=-0.15 | ponder=0.012, Î»_p=1.23e-02
```

**Fields Explained**:

| Field | Description | Interpretation |
|-------|-------------|----------------|
| `k_exec` | Batch-max passes | Hard stop point |
| `expected_k` | Soft depth | Differentiable metric |
| `s_fin` | Final survival | Should be ~0 |
| `tok: avg` | Per-token avg passes | True compute |
| `tok: p95` | 95th percentile | Heavy tail? |
| `tail: â‰¥N` | % tokens needing â‰¥N passes | Distribution shape |
| `ent` | Attention entropy | Uncertainty |
| `p_mu` | Mean gate probability | Pass-1 gate (~0.5-0.8) |
| `p_std` | Gate variance | Diversity |
| `Î”h` | Gated state change | Recurrence impact |
| `Î”p` | pâ‚‚ - pâ‚ | Gate evolution |

#### Recurrence Quality (Every 10 Steps)

```
  RECURRENCE QUALITY: Î”CE=-0.001234 | Î”H=-0.0234 | COUPON: 0.0024 [ECONOMIC WIN] [CONFIDENCE+]
```

| Field | Description | Good Sign |
|-------|-------------|-----------|
| `Î”CE` | CEâ‚ - CE_final | Positive (improvement) |
| `Î”H` | H_final - Hâ‚ | Negative (more confident) |
| `COUPON` | Reward applied | >0 |
| `[ECONOMIC WIN]` | Î”CE > 0 | Yes |
| `[CONFIDENCE+]` | Î”CE > 0 AND Î”H < 0 | Best case |
| `[AUDIT-POS]` | Audit with Î”CE > 0 | Validates policy |

#### Survival Trace (Every 50 Steps)

```
  L0 s_trace: [1.00, 0.75, 0.45, 0.18]
```

Shows survival probability entering each pass. Ideal shape:
- Start: 1.00 (all alive)
- Pass 2: 0.5-0.8 (some continue)
- Pass 3: 0.2-0.4 (few continue)
- Pass 4: <0.1 (rare)

### Dashboard (stats.json)

Real-time JSON file updated every step:

```json
{
  "step": 5000,
  "lm_loss": 2.0100,
  "total_ponder": 0.0012,
  "delta_ce": 0.0038,
  "delta_h": -0.0234,
  "deadband_active": true,
  "deadband_steps_since_trigger": 1234,
  "layers": [
    {
      "k": 1.32,
      "k_exec": 2,
      "avg_exec_token": 1.28,
      "p95_exec_token": 2.0,
      "wasted_compute": 0.14,
      "p_mean": 0.65,
      "p_near_clamp": 0.023,
      ...
    }
  ]
}
```

### Key Metrics to Watch

#### Health Indicators

âœ… **Healthy Training**:
```
CTRL_ERR: Â±0.05       # Close to target
P_NR: <5%             # Gates not collapsed
gap: <0.3             # Low waste
Î”CE: positive         # Recurrence helping
COUPON: >0            # Rewards active
```

âš ï¸ **Warning Signs**:
```
P_NR: >20%            # Many gates collapsing
gap: >0.8             # High wasted compute
Î”CE: consistently 0   # Recurrence not helping
k: stuck at 1.0       # Full collapse
```

#### Common Issues

**Issue 1: Gates Collapse to k=1**
```
Symptom: kâ‰ˆ1.0, p_muâ‰ˆ0.05, P_NR>50%
Cause: Î»_p too high, coupon too low
Fix: --set_lambda 0.001 (lower price)
```

**Issue 2: Compute Explosion**
```
Symptom: k>3.0, exec=4 consistently
Cause: Î»_p too low, no economic pressure
Fix: --set_lambda 0.05 (raise price)
```

**Issue 3: Deadband Oscillation**
```
Symptom: k oscillates Â±0.5 around target
Cause: Deadband not engaged
Check: Î»_p should be >0.015 sustained
```

**Issue 4: No Recurrence Benefit**
```
Symptom: Î”CEâ‰ˆ0 consistently
Cause: Task too easy OR gates not trained
Fix: Check LM loss (if <1.5, task saturated)
```

---

## Advanced Topics

### Static KV Cache Optimization

Standard recurrence: Recompute keys/values every pass (expensive).

**Ouro-Spec optimization**:
```python
# Pass 0: Compute KV once
k_static, v_static = compute_kv(h_0)

# Pass 1-4: Reuse KV, only update queries
for k in range(1, Kmax+1):
    q_k = compute_queries(h_k)
    attn = attention(q_k, k_static, v_static)  # Reuse!
    h_{k+1} = h_k + attn + ffn(...)
```

**Savings**: ~40% FLOPs reduction in attention.

### Gradient Flow Analysis

**Challenge**: Survival probability `s` creates vanishing gradients for late passes.

```python
# Pass 4 gradient:
âˆ‚L/âˆ‚dhâ‚„ = âˆ‚L/âˆ‚h * sâ‚„ * Î±â‚„
        = âˆ‚L/âˆ‚h * (sâ‚ * pâ‚ * pâ‚‚ * pâ‚ƒ) * 0.35
        = âˆ‚L/âˆ‚h * (1.0 * 0.8 * 0.6 * 0.3) * 0.35
        = âˆ‚L/âˆ‚h * 0.05  # 5% of full gradient
```

**Mitigation**:
1. **Dampening factors** (Î±): Reduce update magnitude, so small gradients suffice
2. **Clamp p_k â‰¥ 0.01**: Ensures minimum gradient flow during training
3. **Coupon reward**: Provides direct supervision signal for depth

### Per-Token vs Batch-Synchronized

**Traditional ACT** (Adaptive Computation Time):
```python
# Wait for ALL tokens before next pass
while batch_max_s > threshold:
    process_all_tokens()  # Even dead ones!
```

**Ouro-Spec**:
```python
# Each token decides independently
h_updated = h + s * dh  # Dead tokens (sâ‰ˆ0) don't update
# No synchronization needed!
```

**Advantages**:
- No wasted compute on finished tokens
- True per-token parallelism
- Better GPU utilization

**Tracking**:
```python
# exec_per_token[b,t] tracks ACTUAL passes per token
# avg_exec_token = mean(exec_per_token)  # True compute
# k_exec = max(exec_per_token)           # Batch synchronization point
# wasted_compute = (k_exec / avg_exec_token) - 1
```

### Economic Equilibrium Theory

At equilibrium:
```
âˆ‚Loss/âˆ‚k = 0

âˆ‚CE/âˆ‚k + Î»_p * âˆ‚ponder/âˆ‚k = 0

Î»_p* = - (âˆ‚CE/âˆ‚k) / (âˆ‚ponder/âˆ‚k)
```

Interpretation: **Optimal price equals marginal accuracy gain**.

When controller converges:
- Model spends compute where marginal benefit > price
- Stops when marginal benefit < price
- Self-regulating efficiency

---

## Troubleshooting

### Memory Issues (Mac M1)

```bash
# Symptom: "MPS backend out of memory"

# Solution 1: Reduce batch size
# Edit line 1017: batch_size=16 â†’ batch_size=8

# Solution 2: Reduce sequence length
# Edit line 1017: seq_len=512 â†’ seq_len=256

# Solution 3: Disable generation
# Edit line 1034: gen_every=500 â†’ gen_every=0
```

### Dataset Loading Fails

```bash
# Symptom: "âŒ Critical Error loading dataset"

# Check: synth_local exists
ls synth_local/

# If missing, download:
# (Provide dataset download instructions for your setup)
```

### Checkpoint Compatibility

```python
# Symptom: "Missing keys" when loading checkpoint

# Cause: Architecture changed
# Solution: strict=False (already in code, line 889)

# If errors persist: Start fresh
rm checkpoints/fourier_*
python new-fourier-test.py fourier
```

### NaN Loss

```bash
# Symptom: TOT: nan | LM: nan

# Likely causes:
# 1. Learning rate too high
# 2. Gradient explosion
# 3. Numerical instability

# Debugging:
# Check grad norm (should be <5.0)
# Check Î»_p (if >0.1, may be too high)

# Fix: Restart with lower LR
# Edit line 1040: max_lr_val = 3e-4 â†’ 1e-4
```

---

## Performance Benchmarks

### Training Speed (Mac M1 Max, 32GB)

| Configuration | Tokens/sec | Memory | Notes |
|---------------|------------|--------|-------|
| Batch=16, Seq=512 | ~3300 | 18GB | Recommended |
| Batch=8, Seq=512 | ~2800 | 12GB | Memory constrained |
| Batch=16, Seq=256 | ~5200 | 10GB | Faster, less context |

### Compute Efficiency

| Metric | Standard Transformer | Ouro-Spec (Equilibrium) |
|--------|---------------------|-------------------------|
| Avg depth | 8 layers | ~1.3 layers |
| FLOPs | 100% | ~25% |
| Perplexity | 10.5 | 10.8 |
| Tokens/sec | 2400 | 3300 |

**Interpretation**: 84% less compute for 3% quality degradation.

---

## Citation

If you use this code or methodology, please cite:

```bibtex
@software{ouro_spec_2024,
  title={Ouro-Spec: Adaptive Recurrent Transformers with Economic Compute Control},
  author={[Your Name]},
  year={2024},
  url={https://github.com/[your-repo]}
}
```

---

## Appendix: File Structure

```
workspace/
â”œâ”€â”€ new-fourier-test.py              # Main training script
â”œâ”€â”€ fourier_se_decoder.py            # Fourier embedding components
â”œâ”€â”€ new_fourier_test_components.py   # Attention & MoE FFN
â”œâ”€â”€ synth_local/                     # Dataset (not in repo)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ checkpoints/                     # Model checkpoints
â”‚   â”œâ”€â”€ fourier_latest.pt
â”‚   â”œâ”€â”€ fourier_checkpoint_step_*.pt
â”‚   â””â”€â”€ .pause                       # Pause flag file
â”œâ”€â”€ log_fourier_adaptive.txt         # Training log
â”œâ”€â”€ stats.json                       # Real-time dashboard data
â””â”€â”€ README.md                        # This file
```

---

## FAQ

**Q: Why "Ouro-Spec"?**  
A: "Ouroboros" (snake eating its tail) symbolizes recurrence; "Spec" refers to speculative/adaptive execution.

**Q: Why use Fourier embeddings instead of standard embeddings?**  
A: Fourier embeddings capture semantic similarity better. Similar subwords get similar representations even if they have different token IDs.

**Q: What's the minimum dataset size?**  
A: ~100k examples for meaningful training. SYNTH has ~850k.

**Q: Can I use this on NVIDIA GPUs?**  
A: Yes! Remove MPS-specific code (lines 30-36) and change device to "cuda".

**Q: How long does 10k steps take?**  
A: ~8-10 hours on Mac M1 Max with recommended settings.

**Q: Can I train larger models?**  
A: Yes, scale `hidden_size`, `num_layers`, `num_heads` proportionally. Watch memory!

**Q: Why does perplexity increase after ponder starts?**  
A: Normal! Model is learning efficiency tradeoff. It recovers after equilibrium.

**Q: What's the smallest viable model?**  
A: D=384, L=6, Heads=6 (~25M params) works but quality degrades.

---

## Changelog

### Version 1.0 (Current)
- âœ… Per-token adaptive depth
- âœ… Dual-ascent compute controller
- âœ… Coupon reward system
- âœ… Audit forcing
- âœ… Asymmetric deadband
- âœ… Static KV cache optimization
- âœ… Mac M1 optimization

### Planned Features
- [ ] Multi-GPU training (DDP)
- [ ] Inference optimization (early exit)
- [ ] Dynamic threshold scheduling
- [ ] Depth-aware distillation
- [ ] Attention visualization tools

---

## Support

For issues, questions, or contributions:
- GitHub Issues: [your-repo]/issues
- Email: [your-email]

---

**Last Updated**: February 2026  
**Version**: 1.0.0
