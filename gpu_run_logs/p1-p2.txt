minimal-exp-growth) [ec2-user@ip-172-31-35-243 model-growth]$ python train_tinystories.py
[init] Device: cuda
[init] GPU:    NVIDIA A10G
[dataset] Loading tokenizer (gpt2) …
config.json: 100%|████████████████████████████████████████████████████████████| 665/665 [00:00<00:00, 2.81MB/s]
tokenizer_config.json: 100%|█████████████████████████████████████████████████| 26.0/26.0 [00:00<00:00, 118kB/s]
Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
vocab.json: 100%|█████████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 28.0MB/s]
merges.txt: 100%|███████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 90.1MB/s]
tokenizer.json: 100%|█████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 35.6MB/s]
[dataset] Loading TinyStories …
README.md: 100%|██████████████████████████████████████████████████████████████| 957/957 [00:00<00:00, 5.52MB/s]
[dataset] Tokenizing 50,000 stories …
Token indices sequence length is longer than the specified maximum sequence length for this model (1106 > 1024). Running this sequence through the model will result in indexing errors
[dataset] Done. 11,091,575 total tokens from 50,000 stories.

============================================================
  PHASE 1  —  Dense Model  (single FFN per block)
============================================================

============================================================
  Phase 1 — Dense
  Steps: 1000  |  LR: 0.0003  |  Device: cuda
  Params: 119.36 M
============================================================
  step    0 | loss 11.0278 | 0.8s
  step   50 | loss 5.0828 | 7.9s
  step  100 | loss 4.6713 | 15.2s
  step  150 | loss 4.3402 | 22.4s
  step  200 | loss 3.7129 | 29.7s
  step  250 | loss 3.7511 | 37.0s
  step  300 | loss 3.5612 | 44.3s
  step  350 | loss 3.6087 | 51.6s
  step  400 | loss 3.5347 | 58.9s
  step  450 | loss 3.3019 | 66.2s
  step  500 | loss 3.3585 | 73.5s
  step  550 | loss 3.2562 | 80.8s
  step  600 | loss 3.1814 | 88.1s
  step  650 | loss 3.1031 | 95.4s
  step  700 | loss 2.9408 | 102.7s
  step  750 | loss 2.8202 | 110.0s
  step  800 | loss 3.0502 | 117.3s
  step  850 | loss 3.0706 | 124.6s
  step  900 | loss 2.8414 | 131.9s
  step  950 | loss 3.0074 | 139.2s
  step  999 | loss 2.7902 | 146.4s

────────────────────────────────────────────────────────────
  RESULTS — Phase 1 — Dense
────────────────────────────────────────────────────────────
  Loss:        11.0278  →  2.7902  (dropped 8.2376)
  Total params: 119.36 M

  Sample greedy decoding (40 new tokens each):
  ────────────────────────────────────────────────────────
  Prompt:    "Once upon a time there was"
  Generated: "a little girl named Lily. She loved to play with her toys and her friends. One day, Lily's mommy asked her to help her to buy a new toy bear. Lily was very happy"
  ────────────────────────────────────────────────────────
  Prompt:    "One day a little"
  Generated: "girl named Lily. She was very excited to see the little girl. She was very happy and wanted to see the little girl.

Lily was very sad. She wanted to see the little"
  ────────────────────────────────────────────────────────
  Prompt:    "The boy and the girl"
  Generated: "went to the park. They saw the big man and the boy. The boy was very happy. The boy was very happy. The boy was very happy. The boy was happy. The boy was"
  ────────────────────────────────────────────────────────

✓ Saved dense model to checkpoints/dense_model.pt

============================================================
  TRANSITION  —  Dense → MoE Upcycling
============================================================

  [transition_to_moe]
    Input:  Dense  (d_model=832, layers=7)
    Output: MoE    (8 experts/block, top-2)
    ✓ 7 blocks converted  (8 experts each, router zeroed)

  [Verification] Per-sequence max |logit_dense − logit_moe|:
    Seq 0: 0.00e+00  OK
    Seq 1: 0.00e+00  OK
    Seq 2: 0.00e+00  OK
    Seq 3: 0.00e+00  OK
    Seq 4: 0.00e+00  OK
    Seq 5: 0.00e+00  OK
    Seq 6: 0.00e+00  OK
    Seq 7: 0.00e+00  OK

  Overall max difference: 0.00e+00
  ✓ PASSED — functional equivalence confirmed (< 0.0001)
✓ Saved initial MoE model to checkpoints/moe_model_init.pt

============================================================
  PHASE 2  —  MoE Model  (8 experts, top-2 routing)
============================================================

============================================================
  Phase 2 — MoE
  Steps: 1000  |  LR: 0.0001  |  Device: cuda
  Params: 526.43 M
============================================================
  step    0 | loss 3.2322 | 0.6s
  step   50 | loss 2.7258 | 18.6s
  step  100 | loss 3.1695 | 36.3s
  step  150 | loss 2.9378 | 54.2s
  step  200 | loss 2.4835 | 72.0s
  step  250 | loss 2.9028 | 89.7s
  step  300 | loss 2.6562 | 107.7s
  step  350 | loss 2.5820 | 125.7s
  step  400 | loss 2.6004 | 143.8s
  step  450 | loss 2.6277 | 161.5s
  step  500 | loss 2.5403 | 179.4s
  step  550 | loss 2.5098 | 197.0s
  step  600 | loss 2.8393 | 214.8s
  step  650 | loss 2.5771 | 232.9s
  step  700 | loss 2.6136 | 250.6s
  step  750 | loss 2.3883 | 268.7s
  step  800 | loss 2.6678 | 286.4s
  step  850 | loss 2.2705 | 304.0s
  step  900 | loss 2.4926 | 321.7s
  step  950 | loss 2.3326 | 339.2s
  step  999 | loss 2.5805 | 356.3s

────────────────────────────────────────────────────────────
  RESULTS — Phase 2 — MoE (8 experts, top-2)
────────────────────────────────────────────────────────────
  Loss:        3.2322  →  2.5805  (dropped 0.6517)
  Total params: 526.43 M
  Active params (top-2 of 8): 177.55 M per token

  Sample greedy decoding (40 new tokens each):
  ────────────────────────────────────────────────────────
  Prompt:    "Once upon a time there was"
  Generated: "a little girl named Lily. She loved to play outside in the park with her friends. One day, they decided to go on a trip to the park. Lily was very excited to go on an"
  ────────────────────────────────────────────────────────
  Prompt:    "One day a little"
  Generated: "boy named Timmy went to the park with his mom. They saw a big, red ball. Timmy wanted to play with the ball, but he was too small.

Timmy's"
  ────────────────────────────────────────────────────────
  Prompt:    "The boy and the girl"
  Generated: "were best friends. They liked to play together and have fun.Once upon a time, there was a little boy named Timmy. Timmy loved to play with his toys and run around."
  ────────────────────────────────────────────────────────

✓ Saved final MoE model to checkpoints/moe_model_final.pt
============================================================
  BOUNDARY SUMMARY
============================================================
  Phase 1 final loss  :  2.7902
  Phase 2 first loss  :  3.2322
  Phase 2 final loss  :  2.5805
  Boundary jump       :  0.4420
    (small jump is normal — it's a different random batch,
     NOT a loss spike from the conversion)
  Phase 2 total drop  :  0.6517
============================================================

  Logs saved → checkpoints/training_history.jsonl

✓ Experiment completed successfully!