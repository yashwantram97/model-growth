# Quick Reference Card - 3-Phase Training (100 Steps)

## ğŸš€ Run Command
```bash
python3 train_tinystories.py
```

## â±ï¸ Expected Time (Mac M1/M2)
- Phase 1: ~3 minutes
- Phase 2: ~6 minutes  
- Phase 3: ~18 minutes
- **Total: ~27 minutes**

## ğŸ“Š Model Progression

| Phase | Type | d_model | Layers | Heads | Total Params | Active Params |
|-------|------|---------|--------|-------|--------------|---------------|
| **1** | Dense | 832 | 7 | 8 | 100 M | 100 M (100%) |
| **2** | MoE | 832 | 7 | 8 | 372 M | 139 M (37.4%) |
| **3** | Large MoE | 1664 | 11 | 16 | 1.47 B | 549 M (37.4%) |

## ğŸ¯ Success Indicators

âœ… **Phase 1â†’2 jump**: < 0.05 (normal)  
âœ… **Phase 2â†’3 jump**: < 0.001 (functional preservation!)  
âœ… **Loss dropping**: Each phase shows improvement  
âœ… **No OOM errors**: Memory management working  

## ğŸ“ˆ Expected Output Format

### Phase Start (Dense)
```
Total Parameters:   100.04 M
Active Params/tok:  100.04 M  (Dense model)
```

### Phase Start (MoE)
```
Total Parameters:   371.64 M
Active Params/tok:  138.88 M  (top-2 of 8 experts)
Inactive Params:    232.76 M  (42 experts idle)
Efficiency: 37.4% active per forward pass
```

## ğŸ”§ Configuration (config.py)
```python
steps_phase1: int = 100  # Phase 1 steps
steps_phase2: int = 100  # Phase 2 steps  
steps_phase3: int = 100  # Phase 3 steps
log_every: int = 10      # Log frequency
batch_size: int = 8      # Batch size
```

## ğŸ’¾ Memory Usage (Mac 24GB)

| Phase | Model | Training | Total | Status |
|-------|-------|----------|-------|--------|
| 1 | 0.4 GB | 1.4 GB | ~1.8 GB | âœ… Safe |
| 2 | 1.5 GB | 4.8 GB | ~6.3 GB | âœ… Safe |
| 3 | 5.9 GB | 18.2 GB | ~24.1 GB | âš ï¸ Tight |

## ğŸ†˜ If OOM in Phase 3
```python
# Option 1: Reduce batch size
batch_size: int = 4  # Instead of 8

# Option 2: Smaller growth
scale_factor=1.5     # Instead of 2
extra_layers=2       # Instead of 4
```

## ğŸ“ Output Files (~10 GB total)
```
checkpoints/
â”œâ”€â”€ dense_model.pt          (~400 MB)
â”œâ”€â”€ moe_model_final.pt      (~1.5 GB)
â”œâ”€â”€ large_moe_final.pt      (~5.9 GB)
â””â”€â”€ training_history.jsonl  (~50 KB)
```

## ğŸ“ Key Metrics to Watch

### During Training
- **Step 0 loss**: Initial loss value
- **Step 10, 20...**: Loss trajectory
- **Final loss**: End-of-phase performance

### At Boundaries
- **Phase 1 end**: ~6.8 (typical)
- **Phase 2 start**: ~6.8-6.9 (small jump OK)
- **Phase 3 start**: ~6.2 (should match Phase 2 end!)

### Parameter Reporting
- **Total params**: All weights in memory
- **Active/tok**: Parameters used per token
- **Efficiency**: Active / Total ratio

## ğŸ” Verification Commands

```bash
# Check config
python3 -c "from config import TrainingConfig; print(TrainingConfig())"

# Test imports
python3 -c "from transfer.simple_growth import scale_bilaterally; print('OK')"

# Check GPU/MPS
python3 -c "import torch; print(torch.backends.mps.is_available())"
```

## ğŸ“– Documentation Files

- **PHASE3_BILATERAL_GROWTH.md**: Complete technical docs
- **PHASE3_QUICKSTART.md**: Usage guide
- **TEST_RUN_100_STEPS.md**: Test configuration details
- **CHANGES_SUMMARY.md**: What was changed
- **QUICK_REFERENCE.md**: This file

## ğŸ“ Understanding the Output

### "top-2 of 8 experts"
- 8 experts exist per MoE layer
- Only top-2 are activated per token
- 6 experts are idle (but still in memory)

### "42 experts idle"
- 7 layers Ã— 6 idle experts = 42 idle experts
- These consume memory but not compute

### "Efficiency: 37.4%"
- Only 37.4% of parameters compute per token
- 62.6% are loaded but waiting
- This enables 2.7Ã— capacity at same cost

## âš¡ Quick Troubleshooting

| Issue | Cause | Solution |
|-------|-------|----------|
| OOM in Phase 3 | Model too large | Reduce batch_size to 4 |
| High P2â†’P3 jump | Preservation failed | Check noise_std (try 1e-6) |
| Loss spike | LR too high | Reduce lr_phase3 by 0.5Ã— |
| Slow training | CPU mode | Check MPS is available |

## ğŸ¯ What Success Looks Like

```
BOUNDARY SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Phase 1 final loss      :  6.8234
  Phase 2 first loss      :  6.8456
  Phase 2 final loss      :  6.2145
  Phase 1â†’2 jump          :  0.0222  âœ… Small (normal)

  Phase 3 first loss      :  6.2146
  Phase 3 final loss      :  5.8923  
  Phase 2â†’3 jump          :  0.0001  âœ… TINY (success!)
  Phase 3 total drop      :  0.3223
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**If Phase 2â†’3 jump < 0.001**: ğŸ‰ Bilateral growth working perfectly!

---

**Ready?** Run: `python3 train_tinystories.py`
